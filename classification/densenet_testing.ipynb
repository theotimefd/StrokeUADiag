{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dd0b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "from monai.data import ImageDataset, DataLoader\n",
    "import monai.transforms as transforms\n",
    "from monai.transforms import EnsureChannelFirst, Compose, RandRotate90, Resize, ScaleIntensity\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from utils.custom_transforms import ScaleIntensityFromHistogramPeak, SetBackgroundToZero, SelectChannelsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f7911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT_DIR = \"/home/fehrdelt/bettik/\"\n",
    "ROOT_DIR = \"/bettik/PROJECTS/pr-gin5_aini/fehrdelt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30204843",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"experiment_0\"\n",
    "SUB_EXPERIMENT_NAME = \"exp_0_0\"\n",
    "\n",
    "MODELS_DIR = ROOT_DIR+f\"StrokeUADiag/{EXPERIMENT_NAME}/{SUB_EXPERIMENT_NAME}/models/\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2244c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f7ff56b3390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ddp_bool = False\n",
    "\n",
    "rank = 0\n",
    "world_size = 1\n",
    "device = 0\n",
    "\n",
    "torch.cuda.set_device(device)\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_num_threads(torch.get_num_threads())\n",
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34b4abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [00:28<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_df = pd.read_csv(ROOT_DIR+f\"StrokeUADiag/data_splits_lists/soop/test.csv\")\n",
    "test_files = [{\"img\": img, \"label\": label} for img, label in zip(test_df[\"participant_id\"], test_df[\"high_nihss\"])]\n",
    "for item in test_files:\n",
    "    item[\"img\"] = ROOT_DIR+\"datasets/StrokeUADiag_classification_inputs/stacked_\"+item[\"img\"]+\".nii.gz\"\n",
    "\n",
    "#test_unhealthy_datalist = test_unhealthy_images_path\n",
    "\n",
    "batch_size = 2\n",
    "num_workers = 4\n",
    "\n",
    "test_transforms = Compose([\n",
    "    transforms.LoadImaged(keys=[\"img\"]),\n",
    "    transforms.EnsureChannelFirstd(keys=[\"img\"]),\n",
    "    SelectChannelsd(keys=[\"img\"], selected_channels=[1,3]),\n",
    "    transforms.ResizeWithPadOrCropd(keys=[\"img\"], spatial_size=(128, 128, 128)),\n",
    "    #SetBackgroundToZero()\n",
    "    ])\n",
    "\n",
    "test_ds = monai.data.CacheDataset(data=test_files, transform=test_transforms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if ddp_bool:\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(test_ds, num_replicas=world_size, rank=rank)\n",
    "else:\n",
    "    test_sampler = None\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=batch_size, shuffle=(not ddp_bool), num_workers=num_workers, pin_memory=True, sampler=test_sampler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61701ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=2, out_channels=2).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"{ROOT_DIR}StrokeUADiag/experiment_0/exp_0_5/models/exp_0_5_best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24b4ed1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 32.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_epoch_loss = 0\n",
    "    for step, test_data in enumerate(tqdm(test_loader)):\n",
    "        test_images, test_labels = test_data['img'].to(device), test_data['label'].to(device)\n",
    "        test_outputs = model(test_images)\n",
    "        #print(f\"test_outputs: {test_outputs}\")\n",
    "        #print(f\"test_outputs.argsort(dim=1): {test_outputs.argmax(dim=1)}\")\n",
    "        y_pred_list.extend(test_outputs.argmax(dim=1).cpu().numpy()) #TODO check\n",
    "        y_true_list.extend(test_labels.cpu().numpy())\n",
    "\n",
    "\n",
    "y_pred = np.array(y_pred_list)\n",
    "y_true = np.array(y_true_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dbc5dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13b9c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance\n",
      "\n",
      "{'ROC AUC': '0.63  -  95% CI 0.52-0.77', 'F1': '0.49  -  95% CI 0.26-0.69', 'F2': '0.44  -  95% CI 0.20-0.66', 'Brier loss': '0.30  -  95% CI 0.15-0.43', 'False negatives': '20.77%  -  95% CI 9.43-32.08', 'False positives': '9.54%  -  95% CI 3.77-16.98', 'Sensitivity': '41.84%  -  95% CI 18.72-65.09', 'Specificity': '85.06%  -  95% CI 72.71-95.00', 'PPV': '60.67%  -  95% CI 36.35-85.71', 'NPV': '72.48%  -  95% CI 57.77-86.70', 'LR+': '3.52  -  95% CI 1.15-10.26', 'LR-': '0.69  -  95% CI 0.40-0.96', 'Youden Index': '0.27  -  95% CI 0.04-0.54', 'Average TP': [np.float64(7.905)], 'Average TN': [np.float64(29.03)], 'Average FP': [np.float64(5.055)], 'Average FN': [np.float64(11.01)]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "test_sensitivity = []\n",
    "test_specificity = []\n",
    "test_PPV = []\n",
    "test_NPV = []\n",
    "test_lr_plus = []\n",
    "test_lr_minus = []\n",
    "test_youden_index = []\n",
    "test_tn = []\n",
    "test_fp = []\n",
    "test_fn = []\n",
    "test_tp = []\n",
    "\n",
    "idx = np.arange(len(y_true))\n",
    "\n",
    "for i in range(200): \n",
    "    # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred[pred_idx], y_true=y_true[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred[pred_idx], y_true=y_true[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred[pred_idx], y_true=y_true[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred[pred_idx], y_true=y_true[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y_true[pred_idx], y_pred[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y_true[pred_idx], y_pred[pred_idx])[0,1]\n",
    "    # Sensitivity (Recall) and Specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true[pred_idx], y_pred[pred_idx]).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "    # Positive Predictive Value (PPV) and Negative Predictive Value (NPV)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "    # Likelihood Ratios\n",
    "    lr_plus = sensitivity / (1 - specificity) if (1 - specificity) > 0 else np.nan\n",
    "    lr_minus = (1 - sensitivity) / specificity if specificity > 0 else np.nan\n",
    "\n",
    "    # Youden index (sensitivity + specificity - 1)\n",
    "    youden_index = sensitivity + specificity - 1\n",
    "\n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n",
    "    test_sensitivity.append(sensitivity * 100)  # Convert to percentage\n",
    "    test_specificity.append(specificity * 100)\n",
    "    test_PPV.append(ppv * 100)  # Convert to percentage\n",
    "    test_NPV.append(npv * 100)  # Convert to percentage\n",
    "    test_lr_plus.append(lr_plus)\n",
    "    test_lr_minus.append(lr_minus)\n",
    "    test_youden_index.append(youden_index)\n",
    "    test_tn.append(tn)\n",
    "    test_fp.append(fp)\n",
    "    test_fn.append(fn)\n",
    "    test_tp.append(tp)\n",
    "\n",
    "\n",
    "print(\"Classification performance\\n\")\n",
    "output = {}\n",
    "\n",
    "# Compute the mean and 95% confidence intervals for each metric\n",
    "# 95% confidence intervals are computed using the 2.5th and 97.5th percentiles of the bootstrap samples\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "output[\"ROC AUC\"] = f\"{bootstrap_roc_auc_test_mean:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "output[\"F1\"] = f\"{bootstrap_f1_test_mean:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"F1:              {bootstrap_f1_test_mean:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "output[\"F2\"] = f\"{bootstrap_f2_test_mean:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"F2:              {bootstrap_f2_test_mean:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "output[\"Brier loss\"] = f\"{bootstrap_brier_test_mean:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "output[\"False negatives\"] = f\"{bootstrap_false_neg_test_mean:.2f}%  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%  - 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "output[\"False positives\"] = f\"{bootstrap_false_pos_test_mean:.2f}%  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"False positives: {bootstrap_false_pos_test_mean:.2f}%  -95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_sensitivity = np.mean(test_sensitivity)\n",
    "ci_lower = np.percentile(test_sensitivity, 2.5)\n",
    "ci_upper = np.percentile(test_sensitivity, 97.5)\n",
    "output[\"Sensitivity\"] = f\"{bootstrap_sensitivity:.2f}%  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"Sensitivity:     {bootstrap_sensitivity:.2f}%  -95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_specificity = np.mean(test_specificity)\n",
    "ci_lower = np.percentile(test_specificity, 2.5)\n",
    "ci_upper = np.percentile(test_specificity, 97.5)\n",
    "output[\"Specificity\"] = f\"{bootstrap_specificity:.2f}%  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"Specificity:     {bootstrap_specificity:.2f}%  -95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_PPV = np.mean(test_PPV)\n",
    "ci_lower = np.percentile(test_PPV, 2.5)\n",
    "ci_upper = np.percentile(test_PPV, 97.5)\n",
    "output[\"PPV\"] = f\"{bootstrap_PPV:.2f}%  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"PPV:     {bootstrap_PPV:.2f}%  -95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_NPV = np.mean(test_NPV)\n",
    "ci_lower = np.percentile(test_NPV, 2.5)\n",
    "ci_upper = np.percentile(test_NPV, 97.5)\n",
    "output[\"NPV\"] = f\"{bootstrap_NPV:.2f}%  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"NPV:     {bootstrap_NPV:.2f}%  -95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_lr_plus = np.mean(test_lr_plus)\n",
    "ci_lower = np.percentile(test_lr_plus, 2.5)\n",
    "ci_upper = np.percentile(test_lr_plus, 97.5)\n",
    "output[\"LR+\"] = f\"{bootstrap_lr_plus:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"Likelihood Ratio +: {bootstrap_lr_plus:.2f}  -95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_lr_minus = np.mean(test_lr_minus)\n",
    "ci_lower = np.percentile(test_lr_minus, 2.5)\n",
    "ci_upper = np.percentile(test_lr_minus, 97.5)\n",
    "output[\"LR-\"] = f\"{bootstrap_lr_minus:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"Likelihood Ratio +: {bootstrap_lr_minus:.2f}  -95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_youden_index = np.mean(test_youden_index)\n",
    "ci_lower = np.percentile(test_youden_index, 2.5)\n",
    "ci_upper = np.percentile(test_youden_index, 97.5)\n",
    "output[\"Youden Index\"] = f\"{bootstrap_youden_index:.2f}  -  95% CI {ci_lower:.2f}-{ci_upper:.2f}\"\n",
    "#print(f\"Youden Index: {bootstrap_youden_index:.2f}  -95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "output[\"Average TP\"] = [np.mean(test_tp)]\n",
    "output[\"Average TN\"] = [np.mean(test_tn)]\n",
    "output[\"Average FP\"] = [np.mean(test_fp)]\n",
    "output[\"Average FN\"] = [np.mean(test_fn)]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fcffd9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7f7d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2df5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm_env_kraken",
   "language": "python",
   "name": "ddpm_env_kraken"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
